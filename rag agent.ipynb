{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5647075",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# SETUP INSTRUCTIONS:\n",
    "# 1. Create a .env file / or copy the example .env file in the project root directory\n",
    "# 2. Add your API keys to the .env file (see .env.example for format)\n",
    "# 3. Get your API keys from:\n",
    "#    - LangChain: https://smith.langchain.com/\n",
    "#    - FireCrawl: https://firecrawl.dev/\n",
    "#    - Tavily: https://tavily.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381e8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = 'llama3'\n",
    "# local_llm = 'gemma3:270m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46025bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "# from langchain_community.document_loaders import FireCrawlLoader\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "urls = [\n",
    "    \"https://www.drupal.org/blog?page=1\",\n",
    "    \"https://www.drupal.org/blog?page=2\",\n",
    "    \"https://www.drupal.org/blog?page=3\",\n",
    "    \"https://www.drupal.org/blog?page=4\",\n",
    "    \"https://www.drupal.org/blog?page=5\",\n",
    "    \"https://www.drupal.org/blog?page=6\",\n",
    "    \"https://www.drupal.org/blog?page=7\",\n",
    "    \"https://www.drupal.org/blog?page=8\"\n",
    "]\n",
    "\n",
    "\n",
    "app = FirecrawlApp(api_key=os.environ['FIRECRAWL_API_KEY'])\n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        scraped_data = app.scrape(url=url, formats=[\"markdown\"])\n",
    "        # Check if scraped_data exists and extract content\n",
    "        if scraped_data:\n",
    "            content = None\n",
    "            \n",
    "            # Extract content based on the object type and available attributes\n",
    "            if hasattr(scraped_data, 'page_content'):\n",
    "                content = scraped_data.page_content\n",
    "            elif hasattr(scraped_data, 'content'):\n",
    "                content = scraped_data.content\n",
    "            elif hasattr(scraped_data, 'markdown'):\n",
    "                content = scraped_data.markdown\n",
    "            elif hasattr(scraped_data, 'text'):\n",
    "                content = scraped_data.text\n",
    "            \n",
    "            if content and content.strip():  # Make sure content is not empty\n",
    "                # Store as dictionary first\n",
    "                docs.append({\n",
    "                    'page_content': content,\n",
    "                    'metadata': {'source': url}\n",
    "                })\n",
    "                # print(f\"Successfully added document from {url}, content length: {len(content)}\")\n",
    "            else:\n",
    "                print(f\"No valid content found for {url}\")\n",
    "        else:\n",
    "            print(f\"No data returned for {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "print(f\"Number of documents scraped: {len(docs)}\")\n",
    "# print(docs[:2])  # Print first two documents for verification\n",
    "# split documents\n",
    "# docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Create Document objects from dictionaries and split them\n",
    "doc_objects = [Document(page_content=doc['page_content'], metadata=doc['metadata']) for doc in docs]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(doc_objects)\n",
    "\n",
    "print(f\"Number of documents before filtering: {len(doc_splits)}\")\n",
    "\n",
    "# Filter out complex metadata and ensure proper document formatting\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    # Ensure the document is an instance of Document and has a 'metadata' attribute\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "print(f\"Number of documents after filtering: {len(filtered_docs)}\")\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=GPT4AllEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspect the underlying Chroma collection\n",
    "\n",
    "collection = vectorstore._collection\n",
    "print(f\"Collection details:\")\n",
    "print(f\"- Name: {collection.name}\")\n",
    "print(f\"- Count: {collection.count()}\")\n",
    "\n",
    "# Get a peek at the actual stored data\n",
    "peek_data = collection.peek(limit=2)\n",
    "print(f\"\\nPeek at stored data:\")\n",
    "print(f\"- IDs: {peek_data.get('ids', [])}\")\n",
    "print(f\"- Documents preview: {[doc[:100] + '...' for doc in peek_data.get('documents', [])]}\")\n",
    "print(f\"- Metadatas: {peek_data.get('metadatas', [])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader with LLM\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance of a retrieved document to a user question. \n",
    "  If the document contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "  Give a binary score 'yes' or 'no' based on the relevance of the document to the question. \\n\n",
    "  Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "   <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "   Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "   Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "   input_variables=[\"document\", \"question\"]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"what is going on in Drupal community?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": docs_txt}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02edb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0.1)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "  template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a domain specific chat assistant for question-answering tasks as per the context provided to you.\n",
    "  You are chatting with regular non technical end user who doesn't need to know about which particular context is provided to you. Use the following pieces for retrived context to \n",
    "  answer the question. Please note, only the Question is asked by the end User, the Context however is provided to you automatically, and not by the End user. \\n\n",
    "  If you know the answer, reply in modest tone and layman terms, Don't say something like \"according to the given context or given content, as the end user is not aware\" \\n\n",
    "  If you don't know the answer, just say that you don't know, don't give any detailed reply in that case.\n",
    "  Use five sentences maximum and keep the answer concise without complicating with technical jargon.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "  Question: {question}\n",
    "  Context: {context}\n",
    "  Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "  input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "# post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What's the latest version of Drupal? When did it release\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Web search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "  template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer is grounded \n",
    "  in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported \n",
    "  by a set of facts. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. \n",
    "  <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "  Here are the facts:\n",
    "  \\n ------- \\n\n",
    "  {documents}\n",
    "  \\n ------- \\n\n",
    "  Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "  input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "  template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer\n",
    "  is useful to resolve a question. Give binary score 'yes' or 'no' score to indicate whether the answer is useful\n",
    "  to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "  <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "  Here is the answer:\n",
    "  \\n ------- \\n\n",
    "  {generation}\n",
    "  \\n ------- \\n\n",
    "  Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "  input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd25f8",
   "metadata": {},
   "source": [
    "# Lang Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241767cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: The user's question.\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add web search results\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: bool\n",
    "    documents: List[str]\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains the retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    If any document is not relevant, we will set a flag to run the web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each  doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains the LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG Generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Perform web search to get additional context\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Web Search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_result = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_result = Document(page_content=web_result)\n",
    "    if documents is not None:\n",
    "        documents.append(web_result)\n",
    "    else:\n",
    "        documents = [web_result]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Decide whether to generate an answer or perform a web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All document have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Check for hallucination.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES THE QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS THE QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    \n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "  \"grade_documents\", \n",
    "  decide_to_generate,\n",
    "  {\n",
    "    \"websearch\": \"websearch\",\n",
    "    \"generate\": \"generate\",\n",
    "  },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "  \"generate\",\n",
    "  grade_generation_v_documents_and_question,\n",
    "  {\n",
    "    \"not supported\": \"generate\",\n",
    "    \"useful\": END,\n",
    "    \"not useful\": \"websearch\"\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cef626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What's the latest version of Drupal?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d56da3",
   "metadata": {},
   "source": [
    "### Comparison: Direct Llama3 Query vs Agentic RAG Workflow\n",
    "\n",
    "#### 1. Question\n",
    "```\n",
    "has Drupal 9 reached EOL? what's the latest version of Drupal?\n",
    "```\n",
    "\n",
    "#### 2. Raw (Vanilla) Local Llama3 Answer (No Context / No RAG)\n",
    "RAW output:\n",
    "```\n",
    "As of March 2023, Drupal 8 has reached its End-of-Life (EOL), which means it is no longer receiving security\n",
    "updates or support from the Drupal community.\n",
    "\n",
    "However, Drupal 9 is still actively maintained and supported. The latest version of Drupal is currently **Drupal\n",
    "9.4.2**, released on January 18, 2023. This version includes various bug fixes, security patches, and new features\n",
    "to improve the overall performance and stability of your Drupal website.\n",
    "\n",
    "Here's a brief overview of the current status:\n",
    "\n",
    "* Drupal 8: EOL (End-of-Life) - no longer receiving security updates or support\n",
    "* Drupal 9: Actively maintained and supported - latest version is **9.4.2**\n",
    "\n",
    "If you're currently using Drupal 8, it's recommended to upgrade to Drupal 9 as soon as possible to ensure you\n",
    "receive the latest security patches and features.\n",
    "\n",
    "total duration:       3.891085186s\n",
    "load duration:        44.131536ms\n",
    "prompt eval count:    26 token(s)\n",
    "prompt eval duration: 50.424352ms\n",
    "prompt eval rate:     515.62 tokens/s\n",
    "eval count:           185 token(s)\n",
    "eval duration:        3.796080071s\n",
    "eval rate:            48.73 tokens/s\n",
    "```\n",
    "\n",
    "##### TLDR:\n",
    "```\n",
    "As of March 2023, Drupal 8 has reached its End-of-Life (EOL)...\n",
    "...latest version of Drupal is currently Drupal 9.4.2, released on January 18, 2023.\n",
    "```\n",
    "Issues:\n",
    "- Drupal 8 actually reached EOL in Nov 2021 (hallucination on date).\n",
    "- States latest major is still Drupal 9.x (outdated; Drupal 10 released Dec 2022, Drupal 11 target mid/late 2024).\n",
    "- Fabricated specific patch version/date pairing without evidence.\n",
    "- Fast (≈3.9s total) and cheap, but ungrounded.\n",
    "\n",
    "#### 3. Agentic RAG Llama3 Answer (From graph output)\n",
    "```\n",
    "Drupal 9 has reached its End Of Life (EOL) in November 2023, and the latest major release is Drupal 11 (released mid 2024).\n",
    "```\n",
    "(Paraphrased from value[\"generation\"] stream output; another earlier run produced: “The latest version of Drupal is Drupal 11, released on August 1, 2024.”)\n",
    "Strengths:\n",
    "- Correctly identifies Drupal 9 EOL (Nov 2023).\n",
    "- Advances to current major (Drupal 11) vs stale Drupal 9.x.\n",
    "- Grounded in retrieved Drupal.org blog content + optional web search path.\n",
    "Weaknesses:\n",
    "- Preface like “According to the provided context” slipped through despite prompt guardrails (style refinement needed).\n",
    "- Exact release date may still be hallucinated if not in retrieved chunks (should cite or soften).\n",
    "\n",
    "#### 4. Why RAG Helps Here\n",
    "| Aspect | Vanilla Llama3 | Agentic RAG Graph |\n",
    "|--------|----------------|-------------------|\n",
    "| Freshness | Relies on model pretraining cutoff; stale. | Pulls from scraped Drupal blog pages; can refresh indexes. |\n",
    "| Hallucination Control | None. | Retrieval grading + hallucination + usefulness graders loop. |\n",
    "| Observability | Only raw text & timings. | Structured stages: retrieve → grade → (optional) web search → generate → evaluate. |\n",
    "| Upgrade Path | Needs model retrain for new facts. | Just re-scrape + re-embed. |\n",
    "| Style Control | Minimal (single prompt). | Modular prompts per function (retrieval grading, generation, hallucination check). |\n",
    "| Failure Modes | Confident wrong specifics. | Can detect ungrounded generations and retry / augment with web search. |\n",
    "\n",
    "#### 5. Latency / Cost Trade-off\n",
    "- Vanilla: Single model call (fast).\n",
    "- RAG Graph: Multiple sequential model invocations (retrieval grading loop + generation + evaluators + possible web search). Higher latency and more tokens, but higher factual reliability.\n",
    "\n",
    "#### 6. Key Observations\n",
    "- The biggest factual delta: Vanilla answer outdated & partially incorrect; RAG answer aligns with real lifecycle (Drupal 9 EOL Nov 2023, progression to Drupal 10/11).\n",
    "- The RAG pipeline’s evaluators act as guardrails but still depend on the retrieval containing the needed fact; adding explicit date-focused re-query could further boost precision.\n",
    "- Style prompt needs reinforcement to remove meta phrases.\n",
    "\n",
    "#### 7. Suggested Improvements to Current RAG Notebook\n",
    "- Add a citation builder: attach source URLs for each supporting snippet.\n",
    "- Normalize answer if hallucination grader fails twice: fallback to “I don’t have sufficient grounded data.”\n",
    "- Add a targeted follow-up retrieval if generation mentions a date not present verbatim in supporting documents (date grounding check).\n",
    "- Cache embeddings & reuse vectorstore across runs to cut startup time.\n",
    "- Log per-stage timings for empirical latency comparison.\n",
    "\n",
    "#### 8. When to Use Which\n",
    "- Use Vanilla: exploratory brainstorming, low-stakes, speed prioritized.\n",
    "- Use Agentic RAG: lifecycle/status/versioning queries (like EOL/version), compliance, user-facing factual assistance.\n",
    "\n",
    "#### 9. Takeaway\n",
    "RAG meaningfully upgrades factual accuracy and resilience vs a bare local LLM, at the cost of complexity and latency. For version/EOL queries, the agentic approach is clearly superior and justifies the overhead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
